---
layout: default
title: Research
description: MIRAE, quantum prototypes, and fine-tuning platforms.
order: 4
---

<div class="doing-back">
  <a href="{{ '/' | relative_url }}">← What I'm doing</a>
</div>

<article class="doing-page">
  <h1 class="doing-page-title">Research</h1>
  <p class="doing-page-lead">MIRAE, quantum prototypes, and fine-tuning platforms.</p>
  <div class="doing-page-content">
    <p>A lot of my time goes into research—not in the narrow academic sense only, but in the sense of asking “what if?” and then building a path to find out. Three threads stand out: the MIRAE series, a quantum computing prototype, and a fine-tuning platform.</p>
    <p><strong>MIRAE</strong> is the name I use for an AI architecture focused on <strong>energy-efficient learning</strong>. Today’s large models are powerful but expensive to train and run; I’m interested in how we can change the architecture and the training process so that we get more capability per unit of energy and compute. That means looking at inductive biases, data efficiency, and where we can trim redundancy without losing quality. The aim isn’t to replace big models overnight but to make better use of what we have and to open the door to training and deploying strong models in more constrained settings.</p>
    <p>The <strong>quantum computing prototype</strong> is a separate but related direction. I’m working on a lightweight, scalable design using <strong>photonic crystals</strong>. The idea is to take quantum principles and implement them in a form that’s actually buildable and testable—not a full-scale quantum computer, but a prototype that can scale in a meaningful way and help explore what’s possible at the boundary of classical and quantum. It’s as much about learning how such systems behave as it is about building something others could extend.</p>
    <p>The <strong>fine-tuning platform</strong> is inspired by what Hugging Face did for pretrained models: make them easy to discover, load, and adapt. I want a place where teams can take a base model, plug in their own data and tasks, and get a fine-tuned model without rebuilding the whole stack. That means thinking about data pipelines, training loops, evaluation, and reproducibility—and keeping the UX simple so that researchers and product teams can focus on their problem, not on infra.</p>
    <p>I’m based in India and often share progress on <a href="https://github.com/bhuvanprakash" target="_blank" rel="noopener">GitHub</a> and <a href="https://www.linkedin.com/in/bhuvanprakash/" target="_blank" rel="noopener">LinkedIn</a>. If you’re working on energy-efficient ML, quantum prototypes, or tooling for fine-tuning, I’d be glad to connect and compare notes.</p>
  </div>
</article>
