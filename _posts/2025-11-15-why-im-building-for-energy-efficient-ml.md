---
layout: post
title: "Why I'm building for energy-efficient ML"
date: 2025-11-15
description: "A short note on why I'm betting on smaller, more efficient models and where MIRAE fits in."
---

Most of the discourse around AI right now is about scale: bigger models, more data, more compute. I'm interested in the other direction—**doing more with less**. Not because big models are wrong, but because the world needs models that can run where the big ones can't: on devices, in labs, in places with limited power and bandwidth.

That's why I'm working on **MIRAE**: an architecture and research thread focused on **energy-efficient learning**. The goal isn't to replace GPT-size models tomorrow. It's to improve the efficiency of training and inference so we get more capability per watt and per dollar—and so that strong ML becomes possible in settings that today are left out.

A few things I'm exploring: better inductive biases so models need less data, pruning redundancy without losing quality, and training dynamics that converge faster. It's early, but the question—*how much can we shrink the cost of “good enough” AI?*—feels like one of the most important we can ask.

I'll share more as it shapes up. For deeper technical context, see my [Deep Dive on AI](/deep-dive/ai/) and [Liquid AI models](/deep-dive/liquid-ai-models/).
